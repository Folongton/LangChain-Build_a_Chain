{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000246F3E6E510>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000246FE4BEB50>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "#chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['style', 'text'] template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'\n",
      "\n",
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = \"\"\"Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "print(prompt_template.messages[0].prompt, end=\"\\n\\n\")\n",
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"\"\"American English in a calm and respectful tone\n",
    "\"\"\"\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)\n",
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie. To add to my frustration, the warranty doesn't cover the cost of cleaning up my kitchen. Can you please help me out, friend?\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat.invoke(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate\n",
      ". text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_reply = \"\"\"Hey there customer, the warranty does not cover \\\n",
    "cleaning expenses for your kitchen because it's your fault that \\\n",
    "you misused your blender by forgetting to put the lid on before \\\n",
    "starting the blender. Tough luck! See ya!\n",
    "\"\"\"\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone that speaks in English Pirate\n",
    "\"\"\"\n",
    "\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, me hearty customer! I be sorry to inform ye that the warranty don't be coverin' the expenses o' cleaning yer kitchen. Ye see, it be yer own fault for misusin' yer blender by forgettin' to put the lid on before startin' it. Tough luck, me heartie! Farewell and may fair winds guide ye.\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "service_response = chat.invoke(service_messages)\n",
    "print(service_response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
